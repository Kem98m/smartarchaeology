# Smart Archaeology
A project group to work on developing the technology for autonomous exploration. This work hopes to achieve a means of safer and more accurate mapping of dangerous excavation sites. With sites that are unstable or partially collapsed ruins, we hope to be able to send robots that can document these areas. We do this using a rover and various sensors such as a lidar and depth camera to accurately map and record. This repo hopes to document our efforts and progress for others to base their work on.

Two rovers are being used in this project. A built from scratch rover following the guidelines found in the [f1tenth](http://f1tenth.org/build.html). Much of our inspiration and system breakdown used ideas from [Autonomous Robots Lab](https://www.autonomousrobotslab.com/autonomous-navigation-and-exploration.html).

Alogrithms and project details will be found in their respective folders with the README containing record of how it was used when updated. 

### depthsense
The Intel realsense T265 tracking camera used was tested to work on its own when hooked up with the onboard Nvidia Jetson TX2 that is meant to be used for the homemade rover following the f1tenth guidelines. The camera is also being used for the jackal unit for the same purpose. The goal with the intel camera is to angle it more towards the ground and detect when there are changes in elevation. This is to improve obstacle avoidance by including the capability to avoid holes. Similar to a lidar the depth camera can generate a point cloud of the environment it sees. A ready model for the camera could not be found and it was discovered that the company has not and did not plan to create on a gazebo model. This causes some issues as using a standard camera model in gazebo would not contain the same data to complete a point cloud which would be used for detecting holes and differences in elevation.
### lidar
Separate from modeling the Jackal robot itself in Gazebo was the task of modeling a Velodyne Lidar sensor similar to, if not the same as that which was included with the physical Jackal. Gazebo made it possible to simulate an environment with basic three-dimensional objects to practice using the Lidar sensor to learn about its environment. The lidar that was simulated was a Velodyne HDL-32 which is known not to be the one on the jackal unit in lab, but it was for exercise purposes as the exact model of the lidar in the lab cannot be determined until checked in person. These models work best when exact dimensions are known. For those reasons, the figure below depicts the completed working model of the velodyne lidar. The blue rays are individual lasers that are emitted to create a 3D map of the robot's environment. The lidar attached to the jackal is known to be either a puck or ultra puck.

### Jackal
The jackal is a ready to go robot from clearpath robotics. One in the lab was chosen to work with. All work was done virtually using Gazebo and RVIZ until the unit was operational. From clearpath robotics, they have a library were it is possible to immediately start modifying XMLs and navigation nodes to first test the rover's mapping capabilites and secondly improve it for our purposes. We can control the movement of the bot manually using intereactive markers in RVIZ to rotate or translate the vehicle. RVIZ also provides us with an option to use keyboard for moving the bot through its teleop node. We need to install a ROS package named teleop\textunderscore twist\textunderscore keyboard for this purpose. Running the command for this package presents us with the characters for moving the bot on the screen. This allowed us to map the surrounding area using RVIZ. For getting information from the plugins, we need to make sure that they added in our RVIZ environment. We need to add the \textit{laser} and point it to the node that is publishing information from the camera mounted on the bot. This allows us to get information in RVIZ. The same has to be done with other plugins like lidar and laser.

The motion planning algorithm selected for the robot was to follow that of the iRobot Roomba: iAdapt. Under the influence of this algorithm, the robot would be expected to continue to move in a straight line until it collided with a physical obstruction, such as a wall, that would cause it to cease its forward motion. The robot would then store the point it collided with in the map of its environment to prevent it from making contact with the obstruction once more. Finally, the robot would turn around to face in the direction opposite to the obstruction and move forward, repeating the same process if it collided with another obstruction and so on and so forth. 

The algorithm's implementation featured a turtlebot URDF model instead of a Jackal URDF used in the previous components of the project for ease of implementation because the turtlebot kit's corresponding Gazebo files are supported by ROS maintainers with a myriad of discussion forums online to aid in troubleshooting. At the heart of the motion planning code is the file roomba/obstacle\_avoidance\_turtlebot/src/algo.cpp, which contains the essence of the iAdapt algorithm, albeit without the obstruction location storage portion described earlier. 

### Homemade Rover
The selected computer for this task is a Jetson TX2 which was installed with Ubunutu 18.04 LTS and ROS melodic. The chassis comprised of an RC car base which contained wheels, shocks, and a motor controller, and a board of ABS plastic as a base for the computer and other components. This board was cut using hand tools and a second base was also made by 3D printing PLA. 
A printed circuit board (PCB) was used for the power board of the rover robot to distribute power from its battery to its sensors, motor, and other devices. The reason for this is that several components such as the hokuyo lidar and the camera work on different voltages compared to the battery. Below is an image of the device with all the electrical components. These components were soldered together  using a schematic found with the board and was ready to be integrated in the rover. The TX2 has been left with the development board until the rest of the rover is ready before removing the computer from the development board and on the ABS base. 



